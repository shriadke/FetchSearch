{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (4.34.0)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: datasets in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (2.14.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from transformers) (3.12.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from transformers) (0.3.3)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from transformers) (0.14.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from transformers) (1.26.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from transformers) (0.16.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: requests in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from sentence-transformers) (0.1.99)\n",
      "Requirement already satisfied: scipy in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from sentence-transformers) (1.11.3)\n",
      "Requirement already satisfied: nltk in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from sentence-transformers) (3.8.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from sentence-transformers) (0.16.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from sentence-transformers) (1.3.1)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from sentence-transformers) (2.1.0)\n",
      "Requirement already satisfied: xxhash in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from datasets) (2023.6.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from datasets) (13.0.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from datasets) (3.8.6)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from datasets) (2.1.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from aiohttp->datasets) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.8.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from requests->transformers) (2.0.6)\n",
      "Requirement already satisfied: networkx in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from torch>=1.6.0->sentence-transformers) (3.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from torch>=1.6.0->sentence-transformers) (1.12)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: click in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from nltk->sentence-transformers) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from nltk->sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from torchvision->sentence-transformers) (10.0.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers sentence-transformers datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed, AutoTokenizer, AutoModel\n",
    "from datasets import load_dataset, load_from_disk, load_metric\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import tqdm\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, models, InputExample, losses, evaluation, util\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(30526, 768)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pretrained model \n",
    "model_ckpt = \"msmarco-distilbert-base-v4\"\n",
    "\n",
    "sbert_model = SentenceTransformer(model_ckpt)\n",
    "\n",
    "word_embedding_model = sbert_model._first_module()\n",
    "\n",
    "tokens = [\"[OFF] \", \" [RN] \", \" [CN] \", \" [PCN] \"]\n",
    "word_embedding_model.tokenizer.add_tokens(tokens, special_tokens=True)\n",
    "word_embedding_model.auto_model.resize_token_embeddings(len(word_embedding_model.tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertTokenizerFast(name_or_path='C:\\Users\\shrin/.cache\\torch\\sentence_transformers\\sentence-transformers_msmarco-distilbert-base-v4\\', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t30522: AddedToken(\"[OFF] \", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t30523: AddedToken(\" [RN] \", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t30524: AddedToken(\" [CN] \", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t30525: AddedToken(\" [PCN] \", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sbert_model.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['offer_ext', 'search_query', 'score'],\n",
       "        num_rows: 7314\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['offer_ext', 'search_query', 'score'],\n",
       "        num_rows: 2814\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['offer_ext', 'search_query', 'score'],\n",
       "        num_rows: 1125\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset_id =\"../data/processed/synthetic_pair/synthetic_search_data/\" \n",
    "#dataset = load_dataset(\"shriadke/FetchSearch\")\n",
    "dataset = load_from_disk(dataset_id)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split lengths: [7314, 2814, 1125]\n",
      "Features: ['offer_ext', 'search_query', 'score']\n",
      "Offer Embeddings:\n",
      "[OFF] thomas bagel thins buy 2 [BN] thomas [CN] bread [CN] frozen breakfast [CN] bakery [PCN] pantry [PCN] deli  bakery\n",
      "Queries:\n",
      "meals\n"
     ]
    }
   ],
   "source": [
    "split_lengths = [len(dataset[split])for split in dataset]\n",
    "\n",
    "print(f\"Split lengths: {split_lengths}\")\n",
    "print(f\"Features: {dataset['train'].column_names}\")\n",
    "print(\"Offer Embeddings:\")\n",
    "\n",
    "print(dataset[\"test\"][1][\"offer_ext\"])\n",
    "\n",
    "print(\"Queries:\")\n",
    "\n",
    "print(dataset[\"test\"][1][\"search_query\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have a <class 'list'> of length 7314 containing <class 'sentence_transformers.readers.InputExample.InputExample'>'s.\n"
     ]
    }
   ],
   "source": [
    "train_examples = []\n",
    "train_data = dataset['train']\n",
    "\n",
    "n_examples = dataset['train'].num_rows# // 2# For agility we only 1/2 of our available data\n",
    "\n",
    "for i in range(n_examples):\n",
    "  example = train_data[i]\n",
    "  train_examples.append(InputExample(texts=[example['offer_ext'], example['search_query']], label=float(example[\"score\"])))\n",
    "print(f\"We have a {type(train_examples)} of length {len(train_examples)} containing {type(train_examples[0])}'s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have a <class 'list'> of length 2814 containing <class 'sentence_transformers.readers.InputExample.InputExample'>'s.\n"
     ]
    }
   ],
   "source": [
    "val_examples = []\n",
    "val_data = dataset['val']\n",
    "\n",
    "n_examples = dataset['val'].num_rows# // 2# For agility we only 1/2 of our available data\n",
    "\n",
    "for i in range(n_examples):\n",
    "  example = val_data[i]\n",
    "  val_examples.append(InputExample(texts=[example['offer_ext'], example['search_query']], label=float(example[\"score\"])))\n",
    "print(f\"We have a {type(val_examples)} of length {len(val_examples)} containing {type(val_examples[0])}'s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have a <class 'list'> of length 1125 containing <class 'sentence_transformers.readers.InputExample.InputExample'>'s.\n"
     ]
    }
   ],
   "source": [
    "test_examples = []\n",
    "test_data = dataset['test']\n",
    "\n",
    "n_examples = dataset['test'].num_rows# // 2# For agility we only 1/2 of our available data\n",
    "\n",
    "for i in range(n_examples):\n",
    "  example = test_data[i]\n",
    "  test_examples.append(InputExample(texts=[example['offer_ext'], example['search_query']], label=float(example[\"score\"])))\n",
    "print(f\"We have a {type(test_examples)} of length {len(test_examples)} containing {type(test_examples[0])}'s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
    "val_dataloader = DataLoader(val_examples, shuffle=True, batch_size=16)\n",
    "test_dataloader = DataLoader(test_examples, shuffle=True, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = losses.CosineSimilarityLoss(model=sbert_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING ARGS\n",
    "num_epochs = 2\n",
    "warmup_steps = int(len(train_dataloader) * num_epochs * 0.1) #10% of train data\n",
    "weight_decay = 0.01\n",
    "output_path = \"./models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_evaluator = evaluation.EmbeddingSimilarityEvaluator([],[],[]).from_input_examples(examples=val_examples)\n",
    "test_evaluator = evaluation.EmbeddingSimilarityEvaluator([],[],[]).from_input_examples(examples=test_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4fe9d9af49c4669a751630f346060cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cec6598c8a04057a797ea73fe979775",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/71 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04646b5cd40445989e3d7a81f699c6d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/71 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# change test_dataloader while training\n",
    "sbert_model.fit(train_objectives=[(test_dataloader, loss)], evaluator=val_evaluator, epochs = num_epochs, warmup_steps= warmup_steps, weight_decay=warmup_steps, output_path= output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (0.16.4)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from huggingface_hub) (4.66.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from huggingface_hub) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from huggingface_hub) (4.8.0)\n",
      "Requirement already satisfied: requests in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from huggingface_hub) (2.31.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from huggingface_hub) (3.12.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from huggingface_hub) (23.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from requests->huggingface_hub) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from requests->huggingface_hub) (3.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from requests->huggingface_hub) (2023.7.22)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from requests->huggingface_hub) (2.0.6)\n"
     ]
    }
   ],
   "source": [
    "pip install huggingface_hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3d35b0644a94bf9a398cd223740d86c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_to_hub(\n",
    "    \"shriadke/fetch-search-msmarco-distilbert-base-v4\", \n",
    "    organization=\"\",\n",
    "    train_datasets=[\"shriadke/FetchSearch\"],\n",
    "    exist_ok=True, \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "trained_sbert_model = SentenceTransformer(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    import re\n",
    "    text = str(text).lower()\n",
    "    text = re.sub('[-]+',' ', text)\n",
    "    text = re.sub('[^A-Za-z0-9\\[\\]\\s]+', '', text)   \n",
    "    #text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_data_df = pd.read_csv(\"../data/processed/synthetic_pair/all_synth_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_sents = synthetic_data_df[\"offer_ext\"].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f179322ecf5449e9f04e5fc1827d6f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "882f4917cb5d4833a6a00a2962af5794",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "doc_embd = trained_sbert_model.encode(doc_sents, show_progress_bar=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e52b45b7ce5644179ec25cfc794f2a83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1.000\t[OFF] spend 50 on a full priced new club membership [BN] sams club\n"
     ]
    }
   ],
   "source": [
    "query = \"sams club\"\n",
    "top_k = 1  \n",
    "q_embeddings =  trained_sbert_model.encode([clean_text(query)], show_progress_bar=True)\n",
    "\n",
    "hits = util.semantic_search(q_embeddings, doc_embd, top_k=top_k)\n",
    "hits = hits[0] \n",
    "\n",
    "for hit in hits:\n",
    "    print(\"\\t{:.3f}\\t{}\".format(hit['score'], doc_sents[hit['corpus_id']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The model is just trained on a smaller data size and was not curated on official score labels.\n",
    "## thus it may produce same hit score for all the queries, but internally, it will try to find most plausible hit irrespective of the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fetch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
