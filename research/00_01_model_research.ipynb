{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (4.34.0)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: datasets in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (2.14.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from transformers) (3.12.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from transformers) (0.3.3)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from transformers) (0.14.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from transformers) (1.26.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from transformers) (0.16.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: requests in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from sentence-transformers) (0.1.99)\n",
      "Requirement already satisfied: scipy in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from sentence-transformers) (1.11.3)\n",
      "Requirement already satisfied: nltk in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from sentence-transformers) (3.8.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from sentence-transformers) (0.16.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from sentence-transformers) (1.3.1)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from sentence-transformers) (2.1.0)\n",
      "Requirement already satisfied: xxhash in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from datasets) (2023.6.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from datasets) (13.0.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from datasets) (3.8.6)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from datasets) (2.1.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from aiohttp->datasets) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.8.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from requests->transformers) (2.0.6)\n",
      "Requirement already satisfied: networkx in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from torch>=1.6.0->sentence-transformers) (3.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from torch>=1.6.0->sentence-transformers) (1.12)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: click in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from nltk->sentence-transformers) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from nltk->sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from torchvision->sentence-transformers) (10.0.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers sentence-transformers datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed, AutoTokenizer, AutoModel\n",
    "from datasets import load_dataset, load_from_disk, load_metric\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import tqdm\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, models, InputExample, losses, evaluation, util\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(30526, 768)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pretrained model \n",
    "model_ckpt = \"msmarco-distilbert-base-v4\"\n",
    "\n",
    "sbert_model = SentenceTransformer(model_ckpt)\n",
    "\n",
    "word_embedding_model = sbert_model._first_module()\n",
    "\n",
    "tokens = [\"[OFF] \", \" [RN] \", \" [CN] \", \" [PCN] \"]\n",
    "word_embedding_model.tokenizer.add_tokens(tokens, special_tokens=True)\n",
    "word_embedding_model.auto_model.resize_token_embeddings(len(word_embedding_model.tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertTokenizerFast(name_or_path='C:\\Users\\shrin/.cache\\torch\\sentence_transformers\\sentence-transformers_msmarco-distilbert-base-v4\\', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t30522: AddedToken(\"[OFF] \", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t30523: AddedToken(\" [RN] \", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t30524: AddedToken(\" [CN] \", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t30525: AddedToken(\" [PCN] \", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sbert_model.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['offer_ext', 'search_query', 'score'],\n",
       "        num_rows: 7314\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['offer_ext', 'search_query', 'score'],\n",
       "        num_rows: 2814\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['offer_ext', 'search_query', 'score'],\n",
       "        num_rows: 1125\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset_id =\"../data/processed/synthetic_pair/synthetic_search_data/\" \n",
    "#dataset = load_dataset(\"shriadke/FetchSearch\")  # shriadke/fetch-search-msmarco-distilbert-base-v4\n",
    "dataset = load_from_disk(dataset_id)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split lengths: [7314, 2814, 1125]\n",
      "Features: ['offer_ext', 'search_query', 'score']\n",
      "Offer Embeddings:\n",
      "[OFF] thomas bagel thins buy 2 [BN] thomas [CN] bread [CN] frozen breakfast [CN] bakery [PCN] pantry [PCN] deli  bakery\n",
      "Queries:\n",
      "meals\n"
     ]
    }
   ],
   "source": [
    "split_lengths = [len(dataset[split])for split in dataset]\n",
    "\n",
    "print(f\"Split lengths: {split_lengths}\")\n",
    "print(f\"Features: {dataset['train'].column_names}\")\n",
    "print(\"Offer Embeddings:\")\n",
    "\n",
    "print(dataset[\"test\"][1][\"offer_ext\"])\n",
    "\n",
    "print(\"Queries:\")\n",
    "\n",
    "print(dataset[\"test\"][1][\"search_query\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have a <class 'list'> of length 7314 containing <class 'sentence_transformers.readers.InputExample.InputExample'>'s.\n"
     ]
    }
   ],
   "source": [
    "# Prepare Train DataLoader\n",
    "train_examples = []\n",
    "train_data = dataset['train']\n",
    "\n",
    "n_examples = dataset['train'].num_rows# // 2# For agility we only 1/2 of our available data\n",
    "\n",
    "for i in range(n_examples):\n",
    "  example = train_data[i]\n",
    "  train_examples.append(InputExample(texts=[example['offer_ext'], example['search_query']], label=float(example[\"score\"])))\n",
    "print(f\"We have a {type(train_examples)} of length {len(train_examples)} containing {type(train_examples[0])}'s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have a <class 'list'> of length 2814 containing <class 'sentence_transformers.readers.InputExample.InputExample'>'s.\n"
     ]
    }
   ],
   "source": [
    "# Prepare Val DataLoader\n",
    "val_examples = []\n",
    "val_data = dataset['val']\n",
    "\n",
    "n_examples = dataset['val'].num_rows# // 2# For agility we only 1/2 of our available data\n",
    "\n",
    "for i in range(n_examples):\n",
    "  example = val_data[i]\n",
    "  val_examples.append(InputExample(texts=[example['offer_ext'], example['search_query']], label=float(example[\"score\"])))\n",
    "print(f\"We have a {type(val_examples)} of length {len(val_examples)} containing {type(val_examples[0])}'s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have a <class 'list'> of length 1125 containing <class 'sentence_transformers.readers.InputExample.InputExample'>'s.\n"
     ]
    }
   ],
   "source": [
    "# Prepare Test DataLoader\n",
    "test_examples = []\n",
    "test_data = dataset['test']\n",
    "\n",
    "n_examples = dataset['test'].num_rows# // 2# For agility we only 1/2 of our available data\n",
    "\n",
    "for i in range(n_examples):\n",
    "  example = test_data[i]\n",
    "  test_examples.append(InputExample(texts=[example['offer_ext'], example['search_query']], label=float(example[\"score\"])))\n",
    "print(f\"We have a {type(test_examples)} of length {len(test_examples)} containing {type(test_examples[0])}'s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
    "val_dataloader = DataLoader(val_examples, shuffle=True, batch_size=16)\n",
    "test_dataloader = DataLoader(test_examples, shuffle=True, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = losses.CosineSimilarityLoss(model=sbert_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING ARGS\n",
    "num_epochs = 2\n",
    "warmup_steps = int(len(train_dataloader) * num_epochs * 0.1) #10% of train data\n",
    "weight_decay = 0.01\n",
    "output_path = \"./models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_evaluator = evaluation.EmbeddingSimilarityEvaluator([],[],[]).from_input_examples(examples=val_examples)\n",
    "test_evaluator = evaluation.EmbeddingSimilarityEvaluator([],[],[]).from_input_examples(examples=test_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4fe9d9af49c4669a751630f346060cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cec6598c8a04057a797ea73fe979775",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/71 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04646b5cd40445989e3d7a81f699c6d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/71 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# change test_dataloader while training\n",
    "sbert_model.fit(train_objectives=[(test_dataloader, loss)], evaluator=val_evaluator, epochs = num_epochs, warmup_steps= warmup_steps, weight_decay=weight_decay, output_path= output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "%huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (0.16.4)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from huggingface_hub) (4.66.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from huggingface_hub) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from huggingface_hub) (4.8.0)\n",
      "Requirement already satisfied: requests in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from huggingface_hub) (2.31.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from huggingface_hub) (3.12.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from huggingface_hub) (23.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from requests->huggingface_hub) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from requests->huggingface_hub) (3.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from requests->huggingface_hub) (2023.7.22)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shrin\\anaconda3\\envs\\fetch\\lib\\site-packages (from requests->huggingface_hub) (2.0.6)\n"
     ]
    }
   ],
   "source": [
    "pip install huggingface_hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3d35b0644a94bf9a398cd223740d86c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store model to hub\n",
    "model.save_to_hub(\n",
    "    \"shriadke/fetch-search-msmarco-distilbert-base-v4\", \n",
    "    organization=\"\",\n",
    "    train_datasets=[\"shriadke/FetchSearch\"],\n",
    "    exist_ok=True, \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_sbert_model = SentenceTransformer(\"msmarco-distilbert-base-v4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = torch.load(\"../artifacts/data_transformation/val.pth\")\n",
    "test_dataloader = torch.load(\"../artifacts/data_transformation/test.pth\")\n",
    "\n",
    "val_evaluator = torch.load(\"../artifacts/data_transformation/val_eval.pth\")\n",
    "test_evaluator = torch.load(\"../artifacts/data_transformation/test_eval.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 4 4\n"
     ]
    }
   ],
   "source": [
    "print(len(val_evaluator.sentences1),len(val_evaluator.sentences2),len(val_evaluator.scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here the bi-encoder will find top-k results and feed to cross encoder for fine-grain the results.\n",
    "# That part is left for future work.\n",
    "# Following is an example of evaluaiton of top result for the given query from val set with the help of the cross encoder.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "import numpy as np\n",
    "model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "cross_scores = model.predict([[ doc, query] for query, doc in zip(val_evaluator.sentences1, val_evaluator.sentences2)])\n",
    "# normalize to 0 to 1\n",
    "rescaled_array = (cross_scores-np.min(cross_scores))/(np.max(cross_scores)-np.min(cross_scores))\n",
    "rescaled_array = np.round(rescaled_array,2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val accuracy with cross-encoders :  98.7500003632158\n"
     ]
    }
   ],
   "source": [
    "errors = []\n",
    "for i, score in enumerate(val_evaluator.scores):\n",
    "    error = abs(score - rescaled_array[i] * 100)\n",
    "    errors.append(error)\n",
    "    #print(np_cos_scores, rescaled_array, score, error)\n",
    "print(\"Val accuracy with cross-encoders : \", 100 - sum(errors)/len(errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_sbert_model = SentenceTransformer(\"msmarco-distilbert-base-v4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    import re\n",
    "    text = str(text).lower()\n",
    "    text = re.sub('[-]+',' ', text)\n",
    "    text = re.sub('[^A-Za-z0-9\\[\\]\\s]+', '', text)   \n",
    "    #text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_data_df = pd.read_csv(\"../data/processed/synthetic_pair/all_synth_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_sents = synthetic_data_df[\"offer_ext\"].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac86621a1a0a42bc95973089a402a7cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "doc_embd = trained_sbert_model.encode(doc_sents, show_progress_bar=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc_embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "472ca33e541a4c308859f6fe0f5b3eb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t0.439\t[OFF] tyson products select varieties spend 20 at sams club [BN] ball park frank [CN] packaged meat [CN] frozen beef [PCN] pantry [PCN] frozen meat\n"
     ]
    }
   ],
   "source": [
    "query = \"sams club\"\n",
    "top_k = 1  \n",
    "q_embeddings =  trained_sbert_model.encode([clean_text(query)], show_progress_bar=True)\n",
    "\n",
    "hits = util.semantic_search(q_embeddings, doc_embd, top_k=top_k)\n",
    "hits = hits[0] \n",
    "\n",
    "for hit in hits:\n",
    "    print(\"\\t{:.3f}\\t{}\".format(hit['score'], doc_sents[hit['corpus_id']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t0.439\t[OFF] tyson products select varieties spend 20 at sams club [BN] ball park frank [CN] packaged meat [CN] frozen beef [PCN] pantry [PCN] frozen meat\n",
      "\t0.438\t[OFF] spend 50 on a full priced new club membership [BN] sams club\n",
      "\t0.405\t[OFF] georges farmers market chicken wings at sams club [BN] georges farmers market\n",
      "\t0.400\t[OFF] spend 110 on a full priced new plus membership and receive an additional 10000 points [BN] sams club\n",
      "\t0.199\t[OFF] order online at zaxbyscom [BN] zaxbys\n",
      "\t0.193\t[OFF] queen v the vip  the wingwoman soft touch silicone vibrator at walmart [BN] queen v\n",
      "\t0.179\t[OFF] sign up for the club card or the club card full priced membership new members only [BN] bjs wholesale [CN] cooking  baking [PCN] pantry\n",
      "\t0.116\t[OFF] hagen dazs 28 ounce at grocery stores [BN] haagen dazs [CN] frozen desserts\n",
      "\t0.116\t[OFF] butterball select varieties spend 10 at king soopers [BN] butterball [CN] nut butters  jam [CN] frozen turkey [PCN] pantry [PCN] frozen meat\n",
      "\t0.108\t[OFF] coors light miller lite or vizzy 12 pack [BN] vizzy [CN] hard seltzers sodas waters lemonades  teas [CN] malt beverages [PCN] alcohol\n"
     ]
    }
   ],
   "source": [
    "hits = util.semantic_search(q_embeddings, doc_embd, top_k=10)\n",
    "hits = hits[0] \n",
    "\n",
    "for hit in hits:\n",
    "    print(\"\\t{:.3f}\\t{}\".format(hit['score'], doc_sents[hit['corpus_id']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The model is just trained on a smaller data size and was not curated on official score labels.\n",
    "## thus it may produce same hit score for all the queries, but internally, it will try to find most plausible hit irrespective of the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../data/processed/embeddings/msmacro_sent_embeddings.pkl', \"rb\") as fIn:\n",
    "    stored_data = pickle.load(fIn)\n",
    "    stored_offers = stored_data['offers']\n",
    "    stored_sentences = stored_data['offer_processed']\n",
    "    stored_embeddings = stored_data['embeddings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff4cd81325d4413499dccf9b88b90ea9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "query = \"sams club\"\n",
    "top_k = 1  \n",
    "q_embeddings =  trained_sbert_model.encode([clean_text(query)], show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits = util.semantic_search(q_embeddings, stored_embeddings, top_k=10)\n",
    "hits = hits[0] \n",
    "\n",
    "offers = []\n",
    "scores = []\n",
    "for hit in hits:\n",
    "    offer = stored_offers[hit['corpus_id']]\n",
    "    score = hit['score']\n",
    "    offers.append(offer)    \n",
    "    scores.append(score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Tyson Products, select varieties, spend $20 at Sam's Club\", \"Tyson Products, select varieties, spend $20 at Sam's Club\", \"George's Farmers Market Chicken Wings, at Sam's Club\", 'Spend $50 on a Full-Priced new Club Membership', 'Spend $110 on a Full-Priced new Plus Membership and receive an ADDITIONAL 10,000 points', 'Sign up for The Club Card or The Club+ Card full-priced membership* (New Members Only)', 'Order online at Zaxbys.com', 'QUEEN V® The VIP & The Wingwoman Soft Touch Silicone Vibrator at Walmart', \"Wings OR Cheesy Breadsticks at Casey's\", \"Wings OR Cheesy Breadsticks at Casey's\"]\n",
      "[['[OFF] tyson products select varieties spend 20 at sams club [RN] sams club [BN] ball park frank [CN] packaged meat [PCN] pantry [RCN] cooking  baking packaged seafood nut butters  jam cereal granola  toaster pastries condiments packaged meals  sides soup  broth bread sauces  marinades packaged fruit  applesauce pickled goods dressings packaged meat pasta  noodles packaged vegetables rice  grains', 'sams club'], ['[OFF] georges farmers market chicken wings at sams club [RN] sams club [BN] georges farmers market', 'sams club'], ['[OFF] spend 50 on a full priced new club membership [RN] sams club [BN] sams club', 'sams club'], ['[OFF] spend 110 on a full priced new plus membership and receive an additional 10000 points [RN] sams club [BN] sams club', 'sams club'], ['[OFF] sign up for the club card or the club card full priced membership new members only [RN] bjs wholesale [BN] bjs wholesale [CN] cooking  baking [PCN] pantry [RCN] cooking  baking packaged seafood nut butters  jam cereal granola  toaster pastries condiments packaged meals  sides soup  broth bread sauces  marinades packaged fruit  applesauce pickled goods dressings packaged meat pasta  noodles packaged vegetables rice  grains', 'sams club'], ['[OFF] order online at zaxbyscom [RN] zaxbys [BN] zaxbys', 'sams club'], ['[OFF] queen v the vip  the wingwoman soft touch silicone vibrator at walmart [RN] walmart [BN] queen v', 'sams club'], ['[OFF] wings or cheesy breadsticks at caseys [RN] caseys general store [BN] caseys general store [CN] frozen pizza  pizza snacks [PCN] frozen [RCN] frozen fruits frozen desserts frozen sides frozen meals frozen frozen vegetables ice frozen breads  doughs frozen pizza  pizza snacks frozen breakfast frozen plant based meat frozen appetizers', 'sams club']]\n",
      "[0.5765717029571533, 0.5284336805343628, 0.4511070251464844, 0.4296583831310272, 0.3482372760772705, 0.2132781594991684, 0.18093356490135193, 0.17803266644477844, 0.15760967135429382, 0.14948894083499908]\n"
     ]
    }
   ],
   "source": [
    "print(offers)\n",
    "print(cross_list)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1, 9}\n"
     ]
    }
   ],
   "source": [
    "remove_idx = set()\n",
    "for i in range(len(offers)):\n",
    "    for j in range(i+1, len(offers)):\n",
    "        pair = [offers[i],offers[j]]\n",
    "        \n",
    "        cross_score = np.round(torch.sigmoid(torch.from_numpy(np.array(cross_model.predict(pair)))),3)\n",
    "        #print(str(i), str(j), pair, cross_score)\n",
    "        if cross_score == 1:\n",
    "            #print(\"to be romoved\")\n",
    "            remove_idx.add(j)\n",
    "print(remove_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_list = []\n",
    "filtered_hits = []\n",
    "for idx in range(len(hits)):\n",
    "    if idx not in remove_idx:\n",
    "        cross_list.append([stored_sentences[hits[idx]['corpus_id']], query])\n",
    "        filtered_hits.append(hits[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce_scores = cross_model.predict(cross_list)\n",
    "for idx in range(len(filtered_hits)):\n",
    "    filtered_hits[idx]['cross-encoder_score'] = np.round(torch.sigmoid(torch.from_numpy(np.array(ce_scores[idx]))).numpy().item(),4) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'corpus_id': 0, 'score': 0.4296583831310272, 'cross-encoder_score': 1.02},\n",
       " {'corpus_id': 616, 'score': 0.3482372760772705, 'cross-encoder_score': 23.11},\n",
       " {'corpus_id': 103, 'score': 0.5765717029571533, 'cross-encoder_score': 1.71},\n",
       " {'corpus_id': 232,\n",
       "  'score': 0.4511070251464844,\n",
       "  'cross-encoder_score': 1.0999999999999999},\n",
       " {'corpus_id': 182, 'score': 0.2132781594991684, 'cross-encoder_score': 0.04},\n",
       " {'corpus_id': 318, 'score': 0.15760967135429382, 'cross-encoder_score': 0.0},\n",
       " {'corpus_id': 73, 'score': 0.18093356490135193, 'cross-encoder_score': 0.02},\n",
       " {'corpus_id': 152, 'score': 0.17803266644477844, 'cross-encoder_score': 0.02}]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Re-ranking with Cross-Encoder took seconds\n",
      "Top 5 hits with CrossEncoder:\n",
      "\t23.110\tSpend $50 on a Full-Priced new Club Membership\n",
      "\t1.710\tSpend $110 on a Full-Priced new Plus Membership and receive an ADDITIONAL 10,000 points\n",
      "\t1.100\tTyson Products, select varieties, spend $20 at Sam's Club\n",
      "\t1.020\tGeorge's Farmers Market Chicken Wings, at Sam's Club\n",
      "\t0.040\tSign up for The Club Card or The Club+ Card full-priced membership* (New Members Only)\n",
      "\t0.020\tWings OR Cheesy Breadsticks at Casey's\n",
      "\t0.020\tOrder online at Zaxbys.com\n",
      "\t0.000\tQUEEN V® The VIP & The Wingwoman Soft Touch Silicone Vibrator at Walmart\n"
     ]
    }
   ],
   "source": [
    "#Sort list by CrossEncoder scores\n",
    "filtered_hits = sorted(filtered_hits, key=lambda x: x['cross-encoder_score'], reverse=True)\n",
    "print(\"\\nRe-ranking with Cross-Encoder took seconds\")\n",
    "print(\"Top 5 hits with CrossEncoder:\")\n",
    "offers = []\n",
    "scores = []\n",
    "for hit in filtered_hits:\n",
    "    print(\"\\t{:.3f}\\t{}\".format(hit['cross-encoder_score'], stored_offers[hit['corpus_id']]))\n",
    "    offers.append(stored_offers[hit['corpus_id']])\n",
    "    scores.append(hit['cross-encoder_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Offer</th>\n",
       "      <th>Relevance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Spend $50 on a Full-Priced new Club Membership</td>\n",
       "      <td>23.109999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spend $110 on a Full-Priced new Plus Membershi...</td>\n",
       "      <td>1.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tyson Products, select varieties, spend $20 at...</td>\n",
       "      <td>1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>George's Farmers Market Chicken Wings, at Sam'...</td>\n",
       "      <td>1.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sign up for The Club Card or The Club+ Card fu...</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Offer  Relevance\n",
       "0     Spend $50 on a Full-Priced new Club Membership  23.109999\n",
       "1  Spend $110 on a Full-Priced new Plus Membershi...       1.71\n",
       "2  Tyson Products, select varieties, spend $20 at...        1.1\n",
       "3  George's Farmers Market Chicken Wings, at Sam'...       1.02\n",
       "4  Sign up for The Club Card or The Club+ Card fu...       0.04"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df = pd.DataFrame({\n",
    "            \"Offer\" : offers,\n",
    "            \"Relevance\" : scores\n",
    "        })\n",
    "output_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fetch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
